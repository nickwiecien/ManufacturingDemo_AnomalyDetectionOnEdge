{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"bc0a2baf-1843-454d-ab07-916ba7f8c57b","showTitle":false,"title":""}},"source":["# Sample Notebook Iot Ingestion to Delta\n","Databricks sample notebook showcases to how to ingest data directly from an IoT Hub (Event Hub compatible endpoint) and land in an ADLS Gen2-backed Delta table.\n","\n","This notebook was adapted from the fantastic blog posts linked below:\n","- https://brentonblogs.com/structured-streaming-in-azure-synapse/\n","- https://github.com/BrentonAD/blog-synapse-streaming/blob/main/code/StreamIngestion.ipynb\n","\n","Successful execution of this notebook requires having a registered secret store with the following named secrets included:\n","- `storageaccount`: Name of an Azure storage account (Blob or ADLS Gen2) where delta table data will be stored\n","- `storagekey`: Storage key associated with the aforementioned storage acount. Note: you may optionally choose to use a SAS token here and configure your connection to storage as such.\n","- `eventhub`: Name of an Event Hub endpoint (can be retrieved from IoT Hub deployments) which events will be retrieved from\n","- `iothubconnstr`: Connection string for your IoT Hub/Event Hub resource. In the case of the former, this should be the connection string for the Event Hub compatible endpoint.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["secret_scope = '<YOUR-SECRET-SCOPE-NAME>'"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b19e274b-c194-4f59-84c2-c01698215bae","showTitle":false,"title":""}},"outputs":[],"source":["# Get names of storage account and event hub resources\n","storage_account = dbutils.secrets.get(secret_scope, 'storageaccount')\n","event_hub = dbutils.secrets.get(secret_scope, 'eventhub')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"331c5cd9-0e90-4099-85c1-38692caaa8c0","showTitle":false,"title":""}},"outputs":[],"source":["# Setup access to storage account for temp data when pushing to Synapse\n","spark.conf.set(f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\", dbutils.secrets.get(secret_scope, 'storagekey'))\n","\n","# Setup storage locations for all data\n","ROOT_PATH = f\"abfss://iot@{storage_account}.dfs.core.windows.net/\"\n","BRONZE_PATH = ROOT_PATH + \"raw/\"\n","CHECKPOINT_PATH = ROOT_PATH + \"checkpoints/\"\n","\n","# Retrieve event hub connection string and create configuration\n","connectionString = dbutils.secrets.get(secret_scope, 'iothubconnstr')\n","\n","ehConf = {\n","  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n","'ehName': event_hub\n","}\n","\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\",\"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\",\"true\")\n","\n","# Pyspark and ML Imports\n","import os, json, requests\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import pandas_udf, PandasUDFType"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0fe621be-8189-4d52-9eb8-c824cea7bd14","showTitle":false,"title":""}},"outputs":[],"source":["# Define schema for uploated telemetry\n","schema = 'timestamp timestamp, sensor_00 float,sensor_01 float,sensor_02 float,sensor_03 float,sensor_04 float,sensor_05 float,sensor_06 float,sensor_07 float,sensor_08 float,sensor_09 float,sensor_10 float,sensor_11 float,sensor_12 float,sensor_13 float,sensor_14 float,sensor_15 float,sensor_16 float,sensor_17 float,sensor_18 float,sensor_19 float,sensor_20 float,sensor_21 float,sensor_22 float,sensor_23 float,sensor_24 float,sensor_25 float,sensor_26 float,sensor_27 float,sensor_28 float,sensor_29 float,sensor_30 float,sensor_31 float,sensor_32 float,sensor_33 float,sensor_34 float,sensor_35 float,sensor_36 float,sensor_37 float,sensor_38 float,sensor_39 float,sensor_40 float,sensor_41 float,sensor_42 float,sensor_43 float,sensor_44 float,sensor_45 float,sensor_46 float,sensor_47 float,sensor_48 float,sensor_49 float, machine_status string'\n","\n","iot_stream = (\n","\t# Read from IoT Hubs directly\n","\tspark.readStream.format(\"eventhubs\")                                               \n","\t# Use the Event-Hub-enabled connect string\n","    .options(**ehConf)                                                               \n","\t# Load the data\n","    .load()\n","    # Extract the \"body\" payload from the messages\n","\t.withColumn('reading', F.from_json(F.col('body').cast('string'), schema))        \n","\t# Create a \"date\" field for partitioning\n","    .select('reading.*', F.to_date('reading.timestamp').alias('date'))                           \n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d5fed35e-32af-46f1-8228-d6158678d70e","showTitle":false,"title":""}},"outputs":[],"source":["# Read stream and write to Delta\n","water_pump_data_to_delta = (\n","  iot_stream                                                                    # Filter out turbine telemetry from other data streams\n","    .select('*')                                                                # Extract the fields of interest\n","    .writeStream.format('delta')                                                # Write our stream to the Delta format\n","    .partitionBy('date')                                                        # Partition our data by Date for performance\n","    .option(\"checkpointLocation\", CHECKPOINT_PATH + \"mfg001\")                   # Checkpoint so we can restart streams gracefully\n","    .start(BRONZE_PATH + \"mfg001\")                                              # Stream the data into an ADLS Path\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"445dbebd-be31-4d63-abf5-60655cf603da","showTitle":false,"title":""}},"outputs":[],"source":["# Sleep 60s - implemented for batch jobs\n","import time\n","time.sleep(60)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a4c89382-ee8e-469c-b788-22a073b5f584","showTitle":false,"title":""}},"outputs":[],"source":["# Create Delta table if not exists\n","while True:\n","  try:\n","    spark.sql(f'CREATE TABLE IF NOT EXISTS mfg001 USING DELTA LOCATION \"{BRONZE_PATH + \"mfg001\"}\"')\n","    break\n","  except:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f202f95a-a1f2-4d87-95ff-2c3b753da7a6","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","SELECT COUNT(*) FROM mfg001"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"049d80b3-936d-4a8c-81ae-e0d32a7b10c3","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","SELECT (*) FROM mfg001 ORDER BY timestamp DESC LIMIT 10"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2e66761f-4f35-47f9-b816-b30ff68e8b19","showTitle":false,"title":""}},"outputs":[],"source":["# Stop streaming job and resume on next notebook execution\n","water_pump_data_to_delta.stop()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":409443814085664,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"IoT Ingestion","notebookOrigID":1497428465246580,"widgets":{}},"kernelspec":{"display_name":"Python 3.7.7 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.7"},"vscode":{"interpreter":{"hash":"f2e0b197ae37a172a070d322ccdfd2dc89f3ea78020965c5820be3c5f3a0dbfb"}}},"nbformat":4,"nbformat_minor":0}
